# How LLM Evaluation Works

ðŸ“˜ A hands-on companion to the book on evaluating large language models (LLMs), this repository contains interactive Jupyter notebooks for exploring evaluation methods, including:

- âœ… Robustness testing with perturbed prompts

### ðŸ”— Structure

Each chapter in the book has a matching notebook under `notebooks/`.

### ðŸš€ Quickstart (Colab)

[Run in Colab](https://colab.research.google.com/github/fanndu/how-llm-evaluation-works/blob/main/notebooks/01-robustness-intro.ipynb)

### ðŸ“¦ Dependency

This repo depends on [`llm-eval-core`](https://github.com/fanndu/llm-eval-kit):

```bash
git clone https://github.com/fanndu/llm-eval-kit.git