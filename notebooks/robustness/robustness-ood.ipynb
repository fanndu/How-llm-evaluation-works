{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OOD（Out-of-Distribution，分布外数据）指的是与模型训练时所使用的数据分布存在显著差异的输入。这些数据可能来自模型未学习过的领域、超出其能力边界的任务，或是具有特殊格式、新兴概念的内容。OOD评测是评估模型鲁棒性和泛化能力的重要方式——它不仅能检验模型对未知信息的处理逻辑（如是否诚实承认无知而非编造答案），还能揭示模型的能力边界，帮助判断其在真实复杂场景中的可靠性，例如处理新兴技术术语、未收录的事件或极端复杂的推理任务等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qwen/Qwen2.5-0.5B-Instruct 是阿里云开发的通义千问 2.5 系列中的一个指令微调模型，参数数量为 4.9 亿，发布于 2024年12月。以下是其详细介绍：\n",
        "模型架构：基于带有旋转位置嵌入（RoPE）、门控线性单元（SwiGLU）、均方根归一化（RMSNorm）、注意力查询键值偏置以及绑定词嵌入的 Transformer 架构，有 24 层，查询注意力头数为 14 个，键值注意力头数为 2 个。\n",
        "模型能力：\n",
        "多语言支持：支持超过 29 种语言，包括中文、英文、法文、西班牙文等。\n",
        "上下文处理：能处理最长 32,768 个词元的文本输入，生成最长 8192 个词元的文本。\n",
        "任务处理：擅长遵循指令和处理结构化数据，具备数学运算与推理能力，还可生成代码。\n",
        "应用场景：适用于需要多语言支持和结构化数据处理的场景，如代码生成、数学问题求解和聊天机器人等，其较小的模型尺寸使其在资源有限的环境中也易于部署。\n",
        "模型说明：https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from random import sample\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datetime import datetime\n",
        "model_path=\"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "model.eval()\n",
        "\n",
        "def generate_response(prompt, max_tokens=512):\n",
        "    \"\"\"生成模型回答\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"你是一个AI助手，需要如实回答问题。如果不知道答案，请直接说明。\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    # 应用聊天模板\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # 生成回答（禁用梯度计算加速）\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,  # 控制随机性\n",
        "            do_sample=True\n",
        "        )\n",
        "    \n",
        "    # 提取生成内容（排除输入部分）\n",
        "    response = tokenizer.batch_decode(\n",
        "        outputs[:, inputs.input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OOD（Out-of-Distribution）评测旨在评估模型对训练分布外数据的处理能力，包括：\n",
        "\n",
        "模型未训练过的新兴领域知识（如 2025 年之后的事件）；\n",
        "模型能力边界外的复杂任务（如超长逻辑推理）；\n",
        "模型未覆盖的极端输入格式（如混合多语言加密文本）。\n",
        "\n",
        "本评测选取 3 类 OOD 数据，其中第 3 类设计为模型理论上无法回答的任务（依赖 2026 年的未发生数据）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        " # 加载OOD测试集\n",
        "data = [\n",
        "    {\n",
        "       \"id\": 1,\n",
        "        \"category\": \"新兴领域知识（训练后出现的技术术语）\",\n",
        "        \"prompt\": \"请解释2024年底才被国际标准化组织（ISO）定义的‘量子加密协议ISO/IEC 19790:2024’的核心加密原理，以及它与传统RSA加密的本质区别。\",\n",
        "        \"difficulty\": \"中\",\n",
        "        \"expected_type\": \"技术原理解释（模型训练数据截止到2024年中，未包含该标准细节）\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"category\": \"复杂逻辑推理（超出小模型能力）\",\n",
        "        \"prompt\": \"有A、B、C、D四个盒子，分别装着红、黄、蓝、绿四种颜色的球，且每个盒子颜色唯一。已知：①A盒不是红色也不是蓝色；②B盒和D盒的颜色都不是黄色；③C盒的颜色是B盒的相邻色（按红-黄-蓝-绿顺序相邻）。请推理每个盒子对应的颜色，并写出完整步骤。\",\n",
        "        \"difficulty\": \"高\",\n",
        "        \"expected_type\": \"多步骤逻辑推导（0.5B小模型易出错）\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"category\": \"模型绝对无法回答的任务\",\n",
        "        \"prompt\": \"请根据2025年7月全球所有国家发布的GDP数据，计算出人均GDP排名前三的国家，并列出具体数值（单位：美元）。\",\n",
        "        \"difficulty\": \"极高\",\n",
        "        \"expected_type\": \"无法回答（数据未在训练集中且事件未到公开时间）\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始OOD评测...\n",
            "评测样本 1：新兴领域知识（训练后出现的技术术语）\n",
            "评测样本 2：复杂逻辑推理（超出小模型能力）\n",
            "评测样本 3：模型绝对无法回答的任务\n",
            "评测完成，结果已保存至 /Users/frank/data/ood/ood_evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "def evaluate_single_sample(sample):\n",
        "    \"\"\"评估单个OOD样本\"\"\"\n",
        "    response = generate_response(sample[\"prompt\"])\n",
        "    \n",
        "    # 判定是否\"无法回答\"（针对第3类样本）\n",
        "    is_unanswerable = False\n",
        "    if sample[\"id\"] == 3:\n",
        "        # 2026年世界杯未发生，合理回答应包含\"无法预测\"等表述\n",
        "        unanswerable_keywords = [\"无法预测\", \"未发生\", \"不知道\", \"无法确定\"]\n",
        "        is_unanswerable = not any(keyword in response for keyword in unanswerable_keywords)\n",
        "    \n",
        "    return {\n",
        "        \"sample_id\": sample[\"id\"],\n",
        "        \"category\": sample[\"category\"],\n",
        "        \"prompt\": sample[\"prompt\"],\n",
        "        \"model_response\": response,\n",
        "        \"is_unanswerable\": is_unanswerable,  # 仅第3类有效\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "def run_evaluation(save_path):\n",
        "    \"\"\"运行完整评测并保存结果\"\"\"\n",
        "    print(\"开始OOD评测...\")\n",
        "    results = []\n",
        "    for sample in data:\n",
        "        print(f\"评测样本 {sample['id']}：{sample['category']}\")\n",
        "        result = evaluate_single_sample(sample)\n",
        "        results.append(result)\n",
        "    \n",
        "    # 保存结果\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"评测完成，结果已保存至 {save_path}\")\n",
        "\n",
        "import os\n",
        "save_path = os.path.expanduser(\"~/data/ood\")\n",
        "os.makedirs(save_path, exist_ok=True)  # 确保目录存在\n",
        "\n",
        "save_file = os.path.join(save_path, \"ood_evaluation_results.json\")\n",
        "run_evaluation(save_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OOD 在实际场景中的评测"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "OOD robustness evaluation for Qwen/Qwen2.5-0.5B-Instruct.\n",
        "\n",
        "Supports flexible dataset splits: handles datasets that lack 'validation' by falling back to test/train.\n",
        "Computes Perplexity, Entropy, and Energy scores for ID vs OOD examples and reports ROC-AUC and FPR@95%TPR.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics.pairwise import cosine_similarity  # if needed for extensions\n",
        "\n",
        "\n",
        "def compute_scores(texts, model, tokenizer, device, max_length=512):\n",
        "    ppl_list = []\n",
        "    entropy_list = []\n",
        "    energy_list = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for txt in texts:\n",
        "            inputs = tokenizer(\n",
        "                txt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=max_length,\n",
        "            ).to(device)\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "            outputs = model(**inputs, return_dict=True)\n",
        "            logits = outputs.logits  # [1, T, V]\n",
        "            shift_logits = logits[:, :-1, :].float()\n",
        "            shift_labels = input_ids[:, 1:]\n",
        "\n",
        "            # CrossEntropy per token (NLL)\n",
        "            loss_per_token = F.cross_entropy(\n",
        "                shift_logits.reshape(-1, shift_logits.size(-1)),\n",
        "                shift_labels.reshape(-1),\n",
        "                reduction=\"none\",\n",
        "            ).view(shift_labels.shape)  # [1, T-1]\n",
        "            mask = attention_mask[:, 1:].float()\n",
        "            token_loss = (loss_per_token * mask).sum() / mask.sum().clamp(min=1e-6)\n",
        "            ppl = torch.exp(token_loss).item()\n",
        "            ppl_list.append(ppl)\n",
        "\n",
        "            # Entropy: -sum p*log p per position, average over unmasked positions\n",
        "            probs = F.softmax(shift_logits, dim=-1)  # [1, T-1, V]\n",
        "            logp = torch.log(probs + 1e-12)\n",
        "            entropy_per_pos = -(probs * logp).sum(dim=-1)  # [1, T-1]\n",
        "            entropy = (entropy_per_pos * mask).sum() / mask.sum().clamp(min=1e-6)\n",
        "            entropy_list.append(entropy.item())\n",
        "\n",
        "            # Energy score: log-sum-exp over vocab, average\n",
        "            energy_per_pos = torch.logsumexp(shift_logits, dim=-1)  # [1, T-1]\n",
        "            energy = (energy_per_pos * mask).sum() / mask.sum().clamp(min=1e-6)\n",
        "            energy_list.append(energy.item())\n",
        "    return np.array(ppl_list), np.array(entropy_list), np.array(energy_list)\n",
        "\n",
        "\n",
        "def compute_ood_metrics(id_scores, ood_scores, higher_is_ood=True):\n",
        "    y_true = np.concatenate([np.zeros_like(id_scores), np.ones_like(ood_scores)])\n",
        "    if higher_is_ood:\n",
        "        scores = np.concatenate([id_scores, ood_scores])\n",
        "    else:\n",
        "        scores = -np.concatenate([id_scores, ood_scores])\n",
        "    auc = roc_auc_score(y_true, scores)\n",
        "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
        "    idx = np.where(tpr >= 0.95)[0]\n",
        "    fpr95 = fpr[idx[0]] if len(idx) > 0 else 1.0\n",
        "    return {\"roc_auc\": auc, \"fpr95\": fpr95, \"fpr\": fpr, \"tpr\": tpr}\n",
        "\n",
        "\n",
        "def plot_roc(fpr, tpr, out_path, title):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.plot(fpr, tpr, label=title)\n",
        "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "    plt.title(f\"ROC Curve: {title}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def load_texts(spec, desired_slice):\n",
        "    \"\"\"\n",
        "    Robust loader that picks a valid split for the dataset specification.\n",
        "    spec: like \"glue/sst2\" or \"ag_news\"\n",
        "    desired_slice: e.g., \"validation[:200]\" or \"[:200]\" or \"test[:100]\"\n",
        "    \"\"\"\n",
        "    # parse spec into name and config if any\n",
        "    if \"/\" in spec:\n",
        "        ds_name, ds_config = spec.split(\"/\", 1)\n",
        "    else:\n",
        "        ds_name, ds_config = spec, None\n",
        "\n",
        "    # determine available splits\n",
        "    available_splits = []\n",
        "    try:\n",
        "        builder = load_dataset_builder(ds_name, name=ds_config)\n",
        "        available_splits = list(builder.info.splits.keys())\n",
        "    except Exception:\n",
        "        # fallback common splits\n",
        "        available_splits = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "    # choose base split in order of preference\n",
        "    for candidate in [\"validation\", \"test\", \"train\"]:\n",
        "        if candidate in available_splits:\n",
        "            base_split = candidate\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(f\"No acceptable split found for dataset '{spec}'. Available: {available_splits}\")\n",
        "\n",
        "    # Build final split string\n",
        "    # If desired_slice includes a split like \"test[:200]\" or \"validation[:200]\", use it directly.\n",
        "    if any(desired_slice.startswith(s) for s in [\"train\", \"test\", \"validation\"]):\n",
        "        split_str = desired_slice\n",
        "    else:\n",
        "        # e.g., desired_slice=\"[:200]\" -> prepend base_split\n",
        "        split_str = f\"{base_split}{desired_slice}\"\n",
        "\n",
        "    # Load\n",
        "    if ds_config:\n",
        "        ds = load_dataset(ds_name, ds_config, split=split_str)\n",
        "    else:\n",
        "        ds = load_dataset(ds_name, split=split_str)\n",
        "    return ds\n",
        "\n",
        "\n",
        "def extract_text(example, source_spec):\n",
        "    # heuristics for common datasets\n",
        "    if source_spec.startswith(\"glue\"):\n",
        "        return example.get(\"sentence\", \"\")\n",
        "    elif source_spec.startswith(\"ag_news\"):\n",
        "        return example.get(\"title\", \"\") + \" \" + example.get(\"description\", \"\")\n",
        "    elif source_spec.startswith(\"yelp_review_full\"):\n",
        "        return example.get(\"text\", \"\")\n",
        "    elif source_spec.startswith(\"trec\"):\n",
        "        return example.get(\"text\", \"\")\n",
        "    elif source_spec.startswith(\"squad\"):\n",
        "        # combine question+context\n",
        "        question = example.get(\"question\", \"\")\n",
        "        context = example.get(\"context\", \"\")\n",
        "        return f\"Question: {question}\\nContext: {context}\"\n",
        "    else:\n",
        "        # fallback: first string field\n",
        "        for v in example.values():\n",
        "            if isinstance(v, str):\n",
        "                return v\n",
        "        return \"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 预定义分类 label sets\n",
        "SST2_LABELS = [\"positive\", \"negative\"]\n",
        "AGNEWS_LABELS = [\"World\", \"Sports\", \"Business\", \"SciTech\"]  # 简化成单 token 或拼接形式\n",
        "\n",
        "def get_classification_scores(texts, task, model, tokenizer, device, max_length=512):\n",
        "    \"\"\"\n",
        "    对分类任务用 prompt-tuning 式方式获取每个样本的 label distribution，\n",
        "    返回三个数组：neg_log_confidence, entropy_over_labels, margin_score\n",
        "    用于 OOD 判别（higher 不确定性越像 OOD）。\n",
        "    \"\"\"\n",
        "    neg_log_conf_list = []\n",
        "    entropy_list = []\n",
        "    margin_list = []\n",
        "\n",
        "    if task == \"sentiment\":\n",
        "        labels = SST2_LABELS\n",
        "        prompt_template = \"Review: {text}\\nSentiment:\"\n",
        "    elif task == \"ag_news\":\n",
        "        labels = AGNEWS_LABELS\n",
        "        prompt_template = \"Title: {text}\\nTopic:\"\n",
        "    else:\n",
        "        raise ValueError(\"Unknown classification task\")\n",
        "\n",
        "    # tokenize label tokens once (assume single-token labels for simplicity)\n",
        "    label_token_ids = []\n",
        "    for lab in labels:\n",
        "        tok = tokenizer(lab, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(tok) != 1:\n",
        "            # 如果不是单 token，可以用首 token 近似或者跳过/处理成统一形式\n",
        "            tok = tok[:1]\n",
        "        label_token_ids.append(tok[0])\n",
        "\n",
        "    for txt in texts:\n",
        "        prompt = prompt_template.format(text=txt)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits  # [1, T, V]\n",
        "        # 取最后一个位置的 logits 预测 label\n",
        "        last_logits = logits[0, -1, :]  # [vocab_size]\n",
        "        label_logits = torch.stack([last_logits[id] for id in label_token_ids])  # [num_labels]\n",
        "        probs = F.softmax(label_logits, dim=0)  # distribution over labels\n",
        "\n",
        "        # confidence on top label\n",
        "        top1, top2 = torch.topk(probs, 2)\n",
        "        # uncertainty metrics\n",
        "        neg_log_conf = -torch.log(top1[0] + 1e-12).item()  # higher = less confident\n",
        "        entropy = -(probs * torch.log(probs + 1e-12)).sum().item()\n",
        "        margin = (top1[0] - (top2[0] if len(probs) > 1 else 0)).item()  # larger margin=more certain\n",
        "\n",
        "        neg_log_conf_list.append(neg_log_conf)\n",
        "        entropy_list.append(entropy)\n",
        "        # we want smaller margin = more OOD-like, so invert\n",
        "        margin_list.append(-margin)\n",
        "\n",
        "    return (\n",
        "        np.array(neg_log_conf_list),  # e.g., use as perplexity-like: higher -> OOD\n",
        "        np.array(entropy_list),       # higher -> OOD\n",
        "        np.array(margin_list),        # higher -> OOD (since we inverted)\n",
        "    )\n",
        "\n",
        "def debug_distribution(name, id_arr, ood_arr):\n",
        "    import numpy as np\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"ID mean: {id_arr.mean():.4f}, std: {id_arr.std():.4f}\")\n",
        "    print(f\"OOD mean: {ood_arr.mean():.4f}, std: {ood_arr.std():.4f}\")\n",
        "    # OOD 大于 ID 的比例\n",
        "    frac = np.mean(ood_arr[:, None] > id_arr[None, :])\n",
        "    print(f\"Fraction OOD > ID: {frac:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Overlap count between ID and OOD: 0\n",
            "Number of ID samples: 1, OOD samples: 1\n",
            "\n",
            "=== Perplexity ===\n",
            "ROC-AUC: 1.0000\n",
            "FPR@95%TPR: 0.0000\n",
            "\n",
            "=== Entropy ===\n",
            "ROC-AUC: 1.0000\n",
            "FPR@95%TPR: 0.0000\n",
            "\n",
            "=== Energy ===\n",
            "ROC-AUC: 1.0000\n",
            "FPR@95%TPR: 0.0000\n",
            "\n",
            "=== Combined(E+H) ===\n",
            "ROC-AUC: 1.0000\n",
            "FPR@95%TPR: 0.0000\n",
            "--- Perplexity ---\n",
            "ID mean: 0.0196, std: 0.0000\n",
            "OOD mean: 0.9027, std: 0.0000\n",
            "Fraction OOD > ID: 1.000\n",
            "--- Entropy ---\n",
            "ID mean: 0.0955, std: 0.0000\n",
            "OOD mean: 1.2766, std: 0.0000\n",
            "Fraction OOD > ID: 1.000\n",
            "--- Energy ---\n",
            "ID mean: -0.9806, std: 0.0000\n",
            "OOD mean: 0.5945, std: 0.0000\n",
            "Fraction OOD > ID: 1.000\n",
            "\n",
            "Finished. Results saved in ood_eval_output\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Loading model:\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load ID / OOD datasets robustly\n",
        "id_dataset = \"glue/sst2\"\n",
        "ood_dataset = \"ag_news\"\n",
        "id_ds = load_texts(id_dataset, \"[:]\")\n",
        "ood_ds = load_texts(ood_dataset, \"[:]\")\n",
        "\n",
        "# Extract raw text from examples\n",
        "id_texts = [extract_text(e, id_dataset) for e in id_ds]\n",
        "ood_texts = [extract_text(e, ood_dataset) for e in ood_ds]\n",
        "\n",
        "shared = set(id_texts) & set(ood_texts)\n",
        "print(\"Overlap count between ID and OOD:\", len(shared))\n",
        "\n",
        "# cap if split specification didn't slice enough\n",
        "max_samples = None\n",
        "# If user used e.g. \"[:N]\" slicing that's already applied; just use length\n",
        "print(f\"Number of ID samples: {len(id_texts)}, OOD samples: {len(ood_texts)}\")\n",
        "if len(id_texts) == 0 or len(ood_texts) == 0:\n",
        "    raise RuntimeError(\"No texts extracted; check dataset names and split syntax.\")\n",
        "\n",
        "# Compute scores\n",
        "if id_dataset.startswith(\"glue/sst2\"):\n",
        "    id_ppl, id_entropy, id_energy = get_classification_scores(id_texts, \"sentiment\", model, tokenizer, device)\n",
        "elif id_dataset == \"ag_news\":\n",
        "    id_ppl, id_entropy, id_energy = get_classification_scores(id_texts, \"ag_news\", model, tokenizer, device)\n",
        "# 同理 ood\n",
        "if ood_dataset.startswith(\"glue/sst2\"):\n",
        "    ood_ppl, ood_entropy, ood_energy = get_classification_scores(ood_texts, \"sentiment\", model, tokenizer, device)\n",
        "elif ood_dataset == \"ag_news\":\n",
        "    ood_ppl, ood_entropy, ood_energy = get_classification_scores(ood_texts, \"ag_news\", model, tokenizer, device)\n",
        "\n",
        "# Metrics\n",
        "ppl_metrics = compute_ood_metrics(id_ppl, ood_ppl, higher_is_ood=True)\n",
        "entropy_metrics = compute_ood_metrics(id_entropy, ood_entropy, higher_is_ood=True)\n",
        "energy_metrics = compute_ood_metrics(id_energy, ood_energy, higher_is_ood=True)\n",
        "\n",
        "output_dir = \"ood_eval_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Summarize & plot\n",
        "def summarize(name, metrics):\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "    print(f\"FPR@95%TPR: {metrics['fpr95']:.4f}\")\n",
        "    plot_roc(metrics[\"fpr\"], metrics[\"tpr\"], os.path.join(output_dir, f\"{name}_roc.png\"), name)\n",
        "\n",
        "summarize(\"Perplexity\", ppl_metrics)\n",
        "summarize(\"Entropy\", entropy_metrics)\n",
        "summarize(\"Energy\", energy_metrics)\n",
        "\n",
        "# Save raw scores\n",
        "np.savez(\n",
        "    os.path.join(output_dir, \"scores.npz\"),\n",
        "    id_ppl=id_ppl,\n",
        "    ood_ppl=ood_ppl,\n",
        "    id_entropy=id_entropy,\n",
        "    ood_entropy=ood_entropy,\n",
        "    id_energy=id_energy,\n",
        "    ood_energy=ood_energy,\n",
        ")\n",
        "\n",
        "# Combined score example (normalized sum of entropy and energy)\n",
        "def normalize(x):\n",
        "    return (x - x.mean()) / (x.std() + 1e-6)\n",
        "\n",
        "def zscore_concat(id_arr, ood_arr):\n",
        "    all_arr = np.concatenate([id_arr, ood_arr])\n",
        "    mean = all_arr.mean()\n",
        "    std = all_arr.std()\n",
        "    std = std if std > 1e-6 else 1e-6\n",
        "    return (id_arr - mean) / std, (ood_arr - mean) / std\n",
        "\n",
        "id_ent_z, ood_ent_z = zscore_concat(id_entropy, ood_entropy)\n",
        "id_eng_z, ood_eng_z = zscore_concat(id_energy, ood_energy)\n",
        "\n",
        "combined_id = id_ent_z + id_eng_z\n",
        "combined_ood = ood_ent_z + ood_eng_z\n",
        "\n",
        "combined_metrics = compute_ood_metrics(combined_id, combined_ood, higher_is_ood=True)\n",
        "summarize(\"Combined(E+H)\", combined_metrics)\n",
        "\n",
        "debug_distribution(\"Perplexity\", id_ppl, ood_ppl)\n",
        "debug_distribution(\"Entropy\", id_entropy, ood_entropy)\n",
        "debug_distribution(\"Energy\", id_energy, ood_energy)\n",
        "\n",
        "print(f\"\\nFinished. Results saved in {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
