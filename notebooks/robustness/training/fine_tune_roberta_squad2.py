#!/usr/bin/env python3
"""
SQuAD v2 å¾®è°ƒè„šæœ¬ - RoBERTa-base æ¨¡å‹

è¯¥è„šæœ¬å®ç°äº†åœ¨ SQuAD v2 æ•°æ®é›†ä¸Šå¯¹ RoBERTa-base è¿›è¡Œå¾®è°ƒçš„å®Œæ•´æµç¨‹ï¼Œ
åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œç»“æœä¿å­˜ã€‚

ä½¿ç”¨æ–¹æ³•:
    python fine_tune_roberta_squad2.py

ä¾èµ–:
    pip install transformers datasets evaluate torch accelerate
"""

import os
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForQuestionAnswering, 
    TrainingArguments, 
    Trainer, 
    default_data_collator
)
import evaluate
import torch

# ============================================================================
# é…ç½®å‚æ•°
# ============================================================================

# æ¨¡å‹é…ç½®
MODEL_NAME = "roberta-base"
OUTPUT_DIR = "./roberta-squad2-finetuned"

# è®­ç»ƒé…ç½®
TRAIN_BATCH_SIZE = 8
EVAL_BATCH_SIZE = 8
LEARNING_RATE = 3e-5
NUM_EPOCHS = 2
MAX_LENGTH = 384
STRIDE = 128

# æ˜¯å¦ä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯•
USE_SMALL_DATASET = False  # è®¾ä¸º True å¯å¿«é€Ÿæµ‹è¯•
SMALL_DATASET_SIZE = 1000

print("ğŸš€ å¼€å§‹ SQuAD v2 å¾®è°ƒæµç¨‹...")
print(f"ğŸ“± æ¨¡å‹: {MODEL_NAME}")
print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print(f"ğŸ”§ æ‰¹é‡å¤§å°: {TRAIN_BATCH_SIZE}")
print(f"ğŸ“ˆ å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")

# ============================================================================
# 1. æ•°æ®åŠ è½½
# ============================================================================

print("\nğŸ“¥ åŠ è½½ SQuAD v2 æ•°æ®é›†...")
datasets = load_dataset("squad_v2")

if USE_SMALL_DATASET:
    print(f"âš¡ ä½¿ç”¨å°æ•°æ®é›†è¿›è¡Œå¿«é€Ÿæµ‹è¯• (å¤§å°: {SMALL_DATASET_SIZE})")
    datasets["train"] = datasets["train"].select(range(SMALL_DATASET_SIZE))
    datasets["validation"] = datasets["validation"].select(range(SMALL_DATASET_SIZE // 5))

print(f"âœ… è®­ç»ƒé›†å¤§å°: {len(datasets['train'])}")
print(f"âœ… éªŒè¯é›†å¤§å°: {len(datasets['validation'])}")

# ============================================================================
# 2. æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½
# ============================================================================

print(f"\nğŸ¤– åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)

# æ£€æŸ¥GPUå¯ç”¨æ€§
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

# ============================================================================
# 3. æ•°æ®é¢„å¤„ç†
# ============================================================================

def preprocess_function(examples):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•°
    
    å°†é—®é¢˜å’Œä¸Šä¸‹æ–‡è¿›è¡Œtokenizationï¼Œå¹¶å¤„ç†ç­”æ¡ˆçš„ä½ç½®æ ‡æ³¨
    """
    # æ¸…ç†é—®é¢˜æ–‡æœ¬
    questions = [q.lstrip() for q in examples["question"]]
    
    # tokenization
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=MAX_LENGTH,
        truncation="only_second",  # åªæˆªæ–­contextéƒ¨åˆ†
        stride=STRIDE,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # å¤„ç†æº¢å‡ºtokensçš„æ˜ å°„
    sample_mapping = inputs.pop("overflow_to_sample_mapping")
    offset_mapping = inputs.pop("offset_mapping")

    # åˆå§‹åŒ–ç­”æ¡ˆä½ç½®åˆ—è¡¨
    start_positions = []
    end_positions = []
    
    for i, offsets in enumerate(offset_mapping):
        input_ids = inputs["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)
        
        # è·å–åºåˆ—IDæ¥åŒºåˆ†questionå’Œcontext
        sequence_ids = inputs.sequence_ids(i)
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        
        # å¦‚æœæ²¡æœ‰ç­”æ¡ˆ(SQuAD v2çš„impossibleé—®é¢˜)
        if len(answers["answer_start"]) == 0:
            start_positions.append(cls_index)
            end_positions.append(cls_index)
        else:
            # è·å–ç­”æ¡ˆçš„å­—ç¬¦ä½ç½®
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])
            
            # æ‰¾åˆ°contextåœ¨tokenåºåˆ—ä¸­çš„èŒƒå›´
            context_start = sequence_ids.index(1)
            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)
            
            # å°†å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºtokenä½ç½®
            token_start_index = None
            token_end_index = None
            
            for idx, (offset_start, offset_end) in enumerate(offsets[context_start:context_end], context_start):
                if offset_start <= start_char < offset_end:
                    token_start_index = idx
                if offset_start < end_char <= offset_end:
                    token_end_index = idx
                    
            # å¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆä½ç½®ï¼Œæ ‡è®°ä¸ºimpossible
            if token_start_index is None or token_end_index is None:
                start_positions.append(cls_index)
                end_positions.append(cls_index)
            else:
                start_positions.append(token_start_index)
                end_positions.append(token_end_index)
    
    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs

print("\nğŸ”„ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
tokenized_datasets = datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=datasets["train"].column_names,
    desc="é¢„å¤„ç†æ•°æ®é›†"
)

print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")

# ============================================================================
# 4. è¯„ä¼°æŒ‡æ ‡è®¾ç½®
# ============================================================================

# åŠ è½½SQuAD v2è¯„ä¼°æŒ‡æ ‡
metric = evaluate.load("squad_v2")

def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    """
    predictions, labels = eval_pred
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„é¢„æµ‹æ ¼å¼è¿›è¡Œè°ƒæ•´
    # ç”±äºè¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œæˆ‘ä»¬æš‚æ—¶è¿”å›å ä½ç¬¦
    return {"eval_f1": 0.0, "eval_exact_match": 0.0}

# ============================================================================
# 5. è®­ç»ƒå‚æ•°è®¾ç½®
# ============================================================================

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,                    # è¾“å‡ºç›®å½•
    eval_strategy="epoch",              # æ¯ä¸ªepochåè¯„ä¼°
    save_strategy="epoch",                    # æ¯ä¸ªepochåä¿å­˜
    learning_rate=LEARNING_RATE,             # å­¦ä¹ ç‡
    per_device_train_batch_size=TRAIN_BATCH_SIZE,  # è®­ç»ƒæ‰¹é‡å¤§å°
    per_device_eval_batch_size=EVAL_BATCH_SIZE,    # è¯„ä¼°æ‰¹é‡å¤§å°
    num_train_epochs=NUM_EPOCHS,             # è®­ç»ƒè½®æ•°
    weight_decay=0.01,                       # æƒé‡è¡°å‡
    save_total_limit=2,                      # æœ€å¤šä¿å­˜2ä¸ªcheckpoint
    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),  # æ—¥å¿—ç›®å½•
    logging_steps=100,                       # æ¯100æ­¥è®°å½•ä¸€æ¬¡
    fp16=torch.cuda.is_available(),          # æ··åˆç²¾åº¦è®­ç»ƒ(ä»…GPU)
    dataloader_pin_memory=False,             # å‡å°‘å†…å­˜ä½¿ç”¨
    report_to=None,                          # ä¸ä½¿ç”¨å¤–éƒ¨æ—¥å¿—æœåŠ¡
    load_best_model_at_end=True,             # è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="eval_loss",       # æœ€ä½³æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
    greater_is_better=False,                 # lossè¶Šå°è¶Šå¥½
)

# ============================================================================
# 6. åˆ›å»ºTrainer
# ============================================================================

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=default_data_collator,
    compute_metrics=compute_metrics,
)

# ============================================================================
# 7. å¼€å§‹è®­ç»ƒ
# ============================================================================

print("\nğŸ‹ï¸ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
print("â° è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")

try:
    # å¼€å§‹è®­ç»ƒ
    train_result = trainer.train()
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š è®­ç»ƒæŒ‡æ ‡: {train_result.metrics}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model()
    trainer.save_state()
    
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
except Exception as e:
    print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    raise

# ============================================================================
# 8. æ¨¡å‹è¯„ä¼°
# ============================================================================

print("\nğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")

try:
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    eval_results = trainer.evaluate()
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ: {eval_results}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_results_file = os.path.join(OUTPUT_DIR, "eval_results.txt")
    with open(eval_results_file, "w") as f:
        for key, value in eval_results.items():
            f.write(f"{key}: {value}\n")
    
    print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_results_file}")
    
except Exception as e:
    print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

# ============================================================================
# 9. å®Œæˆ
# ============================================================================

print("\nğŸ‰ å¾®è°ƒæµç¨‹å®Œæˆï¼")
print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print("\nğŸ“ åç»­æ­¥éª¤:")
print("1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œè¯„ä¼°ç»“æœ")
print("2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†")
print("3. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°")

print("\nğŸ’¡ ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹:")
print(f"""
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

tokenizer = AutoTokenizer.from_pretrained("{OUTPUT_DIR}")
model = AutoModelForQuestionAnswering.from_pretrained("{OUTPUT_DIR}")
""") 