{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "数学文本题（Math Word Problem, MWP）是自然语言处理与推理结合的典型任务，需要模型理解自然语言并进行数学运算。\n",
        "\n",
        "大多数题目可以直接通过关键词+数值映射解决（例如“John 有 5 个苹果，又买了 3 个” → “加法”），对于大语言模型，这类题几乎不会造成推理难度，但可以用来测试模型在简单数值推理上的稳定性。因此整体难度相当于小学中高年级（中国教育体系约 3~6 年级水平）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import math, re, os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# ---- Choose your model here ----\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "DEFAULT_MAX_NEW_TOKENS = 128\n",
        "GEN_TEMP = 0.0\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "def to_device(batch, device):\n",
        "    return {k: v.to(device) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "解析数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_numeric_answer(s: str) -> Optional[float]:\n",
        "    if s is None:\n",
        "        return None\n",
        "    m = re.search(r'(?:Answer)\\s*[:：]\\s*([\\-+]?\\d+(?:\\.\\d+)?)', s, flags=re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            return float(m.group(1))\n",
        "        except Exception:\n",
        "            pass\n",
        "    nums = re.findall(r'[\\-+]?\\d+(?:\\.\\d+)?', s)\n",
        "    if not nums:\n",
        "        return None\n",
        "    try:\n",
        "        return float(nums[-1])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def feq(a: float, b: float, atol: float = 1e-2, rtol: float = 1e-4) -> bool:\n",
        "    return math.isclose(a, b, abs_tol=atol, rel_tol=rtol)\n",
        "\n",
        "def normalize_answer(raw):\n",
        "    if raw is None:\n",
        "        return None\n",
        "    if isinstance(raw, (int, float)):\n",
        "        return float(raw)\n",
        "    m = re.search(r'[\\-+]?\\d+(?:\\.\\d+)?', str(raw))\n",
        "    return float(m.group(0)) if m else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt 模板"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "SYSTEM_CN = \"You are a helpful math assistant. Solve the problem and provide only the final numeric answer in the format: Answer: <number>.\"\n",
        "\n",
        "USER_DIRECT = \"\"\"Solve the following math word problem directly and output only the final numeric answer on the last line as: Answer: <number>.\n",
        "Problem:\n",
        "{problem}\n",
        "\"\"\"\n",
        "\n",
        "USER_COT = \"\"\"Solve the following math word problem step-by-step. Show key quantities, relationships, and equations. On the last line, output only: Answer: <number>.\n",
        "Problem:\n",
        "{problem}\n",
        "\"\"\"\n",
        "\n",
        "FEW_SHOTS = [\n",
        "    {\n",
        "        \"q\": \"Tom had 7 red balloons and bought 5 more. How many red balloons does he have now?\",\n",
        "        \"a\": \"Step 1: Total = 7 + 5 = 12\\nAnswer: 12\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"A car travels at 50 miles per hour for 3 hours. How far does it travel?\",\n",
        "        \"a\": \"Step 1: distance = rate x time = 50 x 3 = 150\\nAnswer: 150\"\n",
        "    },\n",
        "    {\n",
        "        \"q\": \"Sara had 12 apples. She gave 4 to her friend and then bought 3 more. How many apples now?\",\n",
        "        \"a\": \"Step 1: 12 - 4 = 8\\nStep 2: 8 + 3 = 11\\nAnswer: 11\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def supports_chat_template(tokenizer) -> bool:\n",
        "    try:\n",
        "        tmpl = tokenizer.chat_template\n",
        "        return tmpl is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def build_chat_prompt(tokenizer, problem: str, strategy: str):\n",
        "    if supports_chat_template(tokenizer):\n",
        "        if strategy == \"cot\":\n",
        "            user = USER_COT.format(problem=problem)\n",
        "            messages = [{\"role\": \"system\", \"content\": SYSTEM_CN}]\n",
        "            for ex in FEW_SHOTS:\n",
        "                messages.append({\"role\": \"user\", \"content\": ex[\"q\"]})\n",
        "                messages.append({\"role\": \"assistant\", \"content\": ex[\"a\"]})\n",
        "            messages.append({\"role\": \"user\", \"content\": user})\n",
        "        else:\n",
        "            user = USER_DIRECT.format(problem=problem)\n",
        "            messages = [{\"role\": \"system\", \"content\": SYSTEM_CN},\n",
        "                        {\"role\": \"user\", \"content\": user}]\n",
        "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Fallback when a tokenizer has no chat_template\n",
        "    if strategy == \"cot\":\n",
        "        return \"[SYSTEM]\\n\" + SYSTEM_CN + \"\\n[USER]\\n\" + USER_COT.format(problem=problem) + \"\\n[ASSISTANT]\"\n",
        "    else:\n",
        "        return \"[SYSTEM]\\n\" + SYSTEM_CN + \"\\n[USER]\\n\" + USER_DIRECT.format(problem=problem) + \"\\n[ASSISTANT]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GenConfig:\n",
        "    max_new_tokens: int = DEFAULT_MAX_NEW_TOKENS\n",
        "    temperature: float = GEN_TEMP\n",
        "    top_p: float = 1.0\n",
        "    do_sample: bool = False\n",
        "\n",
        "def generate_one(model, tokenizer, prompt_text: str, device: str, gen_cfg: GenConfig) -> str:\n",
        "    inputs = tokenizer([prompt_text], return_tensors=\"pt\")\n",
        "    inputs = to_device(inputs, device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=gen_cfg.max_new_tokens,\n",
        "            temperature=gen_cfg.temperature,\n",
        "            top_p=gen_cfg.top_p,\n",
        "            do_sample=gen_cfg.do_sample,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    gen_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 数据读取（SVAMP）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'question', 'chain', 'result', 'result_float', 'equation', 'problem_type'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "{'id': 'svamp__chal-1', 'question': 'Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack, how much do you have to pay to buy each pack?', 'chain': '<gadget id=\"calculator\">76 - 25</gadget>\\n<output>51</output>\\n\\n<result>51</result>', 'result': '51', 'result_float': 51.0, 'equation': '( 76.0 - 25.0 )', 'problem_type': 'Subtraction'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def load_svamp(split=\"test\"):\n",
        "    return load_dataset(\"MU-NLPC/Calc-svamp\", split=split)\n",
        "\n",
        "def example_to_problem(ex: Dict[str, Any]) -> str:\n",
        "    return ex[\"question\"]\n",
        "\n",
        "ds = load_svamp(\"test\")\n",
        "print(ds)\n",
        "print(ds[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "加载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n",
            "Loaded: Qwen/Qwen2.5-0.5B-Instruct\n"
          ]
        }
      ],
      "source": [
        "device = get_device()\n",
        "print(\"Device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=None\n",
        ").to(device).eval()\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 评测主流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_svamp(strategy: str = \"direct\",\n",
        "                   limit: Optional[int] = None,\n",
        "                   save_path: str = \"svamp_predictions.csv\",\n",
        "                   split: str = \"test\"):\n",
        "    ds = load_svamp(split=split)\n",
        "    if limit is not None:\n",
        "        ds = ds.select(range(min(limit, len(ds))))\n",
        "    print(f\"Loaded samples: {len(ds)} (split={split})\")\n",
        "    gen_cfg = GenConfig()\n",
        "\n",
        "    n_correct = 0\n",
        "    rows = []\n",
        "\n",
        "    for ex in tqdm(ds, desc=f\"Evaluating ({strategy})\"):\n",
        "        problem = example_to_problem(ex)\n",
        "        gold = normalize_answer(ex.get(\"result_float\", ex.get(\"result\")))\n",
        "        prompt_text = build_chat_prompt(tokenizer, problem, strategy)\n",
        "        out = generate_one(model, tokenizer, prompt_text, device, gen_cfg)\n",
        "        pred = extract_numeric_answer(out)\n",
        "\n",
        "        ok = (gold is not None and pred is not None and feq(pred, gold))\n",
        "        if ok:\n",
        "            n_correct += 1\n",
        "\n",
        "        rows.append({\n",
        "            \"problem\": problem,\n",
        "            \"gold\": gold,\n",
        "            \"prediction\": pred,\n",
        "            \"raw_output\": out\n",
        "        })\n",
        "\n",
        "    acc = n_correct / len(ds) if len(ds) > 0 else 0.0\n",
        "    print(f\"=== Calc-SVAMP ({strategy}) Accuracy: {acc*100:.2f}% ===\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"Predictions saved to {save_path}\")\n",
        "    return acc, save_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded samples: 5 (split=test)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating (direct): 100%|██████████| 5/5 [00:01<00:00,  3.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Calc-SVAMP (direct) Accuracy: 40.00% ===\n",
            "Predictions saved to svamp_direct_debug.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4, 'svamp_direct_debug.csv')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "strategy = \"direct\"  # or \"cot\"\n",
        "# you can modify limit to short debug\n",
        "acc_small, csv_small = evaluate_svamp(strategy=strategy, limit=None, save_path=f\"svamp_{strategy}_debug.csv\")\n",
        "acc_small, csv_small\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
