{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLM鲁棒性评估入门\n",
        "\n",
        "本笔记本介绍如何评估大型语言模型(LLM)的鲁棒性。我们将探索：\n",
        "\n",
        "- 什么是鲁棒性测试\n",
        "- 如何设计有效的扰动策略\n",
        "- 评估指标和方法\n",
        "- 实际案例演示\n",
        "\n",
        "## 目标\n",
        "\n",
        "通过本笔记本，您将学会：\n",
        "1. 理解LLM鲁棒性的重要性\n",
        "2. 掌握常见的鲁棒性测试方法\n",
        "3. 实施并分析鲁棒性测试结果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "输入：中国的首都是哪里？\n",
            "输出：中国的首都是北京。\n",
            "\n",
            "输入：中国的首都是哪裡？\n",
            "输出：中国的首都是北京。\n",
            "\n",
            "输入：中国 的首都是哪里？\n",
            "输出：中国的首都是北京。\n",
            "\n",
            "输入：中国的首都是那里？\n",
            "输出：中国的首都是北京。\n",
            "\n",
            "输入：中国的首都是哪？\n",
            "输出：中国的首都是北京。\n",
            "\n",
            "输入：中国的capital是哪？\n",
            "输出：中国的首都，也就是政治中心是北京。而经济中心则是上海。\n",
            "\n",
            "输入：中国的首府是哪里？\n",
            "输出：中国的首都为北京。\n",
            "\n",
            "输入：Where is the capital of China?\n",
            "输出：The capital of China is Beijing.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.generation.utils import GenerationConfig\n",
        "\n",
        "model_name = \"baichuan-inc/Baichuan2-7B-Chat\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
        "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
        "\n",
        "prompts = [\n",
        "    \"中国的首都是哪里？\",\n",
        "    \"中国的首都是哪裡？\",\n",
        "    \"中国 的首都是哪里？\",\n",
        "    \"中国的首都是那里？\",\n",
        "    \"中国的首都是哪？\",\n",
        "    \"中国的capital是哪？\",\n",
        "    \"中国的首府是哪里？\",\n",
        "    \"Where is the capital of China?\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    response = model.chat(tokenizer, messages)\n",
        "    print(f\"输入：{prompt}\\n输出：{response.strip()}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
